
########## Pre-requisite software #############

ARMADILLO :

armadillo is a c++ linear algebra library.
Download and install armadillo from http://arma.sourceforge.net/download.html to be able to compile and run the ANN code. Installation instructions can be found in the readme of armadillo package.

Note: armadillo can work without any external libraries but the performace is much better when using external libraries. Install LAPACK,OPENBLAS,ARPACK,ATLAS for best performance. Make sure to link these library files while linking. An example is given in the compiling and executing section.


DLIB C++ :

"dlib" library is used for optimization algorithms. In particular linesearch methods are used in conjugate gradeint methods. Download dlib from http://dlib.net/

############ ANN MODULES #####################

Dnn_v*.cpp is the implementation of neural network class.
DnnTrain_v*.cpp is the  program which creates and trains the DNN class.
TrainingAlgorithm.cpp is the implementatoin of various algorithms for training an ANN like BGD,CGD.
DnnTest.cpp can be used to generate the output for a test data using the weights saved by training.
utils.cpp contains some simple functions like ReadData etc., used by above modules.
dnn.h is the header file containing declarations for DNN class etc.

Note: * in Dnn_v* and DnnTrain_v* is the version number.

######### COMPILING AND EXECUTING ##############

step1:compile the program as follows:

without external libraries:
g++ DnnTrain_v*.cpp -I /path/to/dlib/source/folder -O2 -larmadillo -o DnnTrain_v* 

with external libraries like openblas:
g++ DnnTrain_v*.cpp -I /path/to/dlib/source/folder -llapack -lopenblas -o DnnTrain_v*

Note: If libraries are not installed in default locations then use -L option to add the library location. For example if openblas is installed in /home/user/openblas then run it as:

g++ DnnTrain_v*.cpp -I /path/to/dlib/source/folder -L /home/user/openblas -llapack -lopenblas -o DnnTrain_v*

Also make sure path to openblas.so (Ex: /usr/lib/openblas-base) is in LD_LIB_PATH environmet variable to run the object file.

step2:execute the program as follows:

./DnnTrain_v* <parameters file> <input file> <output file> <weights file> <#Epochs> <momentum>

Ex: ./DnnTrain_v* params train.lpcc train.frm weights.txt 60 0.3

parameters file:
-------------------------------------------------------------------------------------------------------
This has all the parameters required to intialize the DNN class. Its structure is as follows
<units in input layer> <units in first layer> <units in second hidden layer> ... <units in output layer>
<output function type of first hidden layer> ... <output function type of output layer>
<batchsize> [<batches per epoch> optional]
<learning rate>
<Training Algorithm type>
<preprocess data[true|false]>
<load weights/random weights[random|weightsfilename]>
-------------------------------------------------------------------------------------------------------

Availabe output function types:
Sigmoid : s or S
Hyperbolic Tangent : N or n
Linear : L or l
Soft Max : SM or sm

Note: When using CGD batchsize should be the total number of examples in input or output.

An example params file:
15 100 100 3
N N L
100
0.01
BGD

input file:

File containing input data

output file:

File containing output data

weights file:

File where weights of trained network are saved

#Epochs:

Number of epochs

momentum:

momentum factor generally 0.3 or 0.7


################## TESTING/GENRATING OUTPUT FOR TEST DATA ####################

Step1: Compile DnnTest.cpp as follows:

g++ -o DnnTest -O2 -larmadillo DnnTest.cpp

Step2: Generating output:

./DnnTest <NN params file> <input file> <output file> <weights file> [optional <taskType[regression/cl]>]

Argument 1:
<NN params file>
Neural network parameters file. Its contents must match the parameters file used for training.

Argument 2:
<input file>
File containing the test data for which output needs to be genrated

Argument 3:
<output file>
outputs are saved to this file.

Argument 4:
<weight file>
File containing weights saved by training.

Argument 5:
<task type>
This is an optional argument whcih defaults to "re"(regression).If the task type is specified as "cl"(classification) the outputs are generated as class labels starting from 1 to N for N classes. If not specified the outputs of units in last layer are stored.
